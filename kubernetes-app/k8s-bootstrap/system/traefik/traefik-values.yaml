# @format
# Traefik Helm Values — DaemonSet with hostNetwork (Hybrid-HA)
#
# Compatible with Traefik Helm chart v3.x (traefik/traefik)
#
# Ensures every node in the kubeadm cluster listens on ports 80/443,
# enabling seamless EIP failover between nodes. When the EIP moves to
# a different node, Traefik is already running and serving traffic.
#
# Key design decisions:
#   - DaemonSet (not Deployment): 1 Traefik pod per node
#   - hostNetwork: true — binds directly to node ethernet (no kube-proxy hop)
#   - dnsPolicy: ClusterFirstWithHostNet — resolves K8s services in host net mode
#   - Control plane toleration — Traefik must also run on the control plane node
#   - Prometheus metrics on port 9100 (matches existing scrape targets)
#   - OTLP tracing to Tempo (preserves existing observability)
#
# Install: helm install traefik traefik/traefik -n kube-system -f traefik-values.yaml
# Chart:   https://traefik.github.io/charts

# DaemonSet ensures one Traefik pod per node
deployment:
  kind: DaemonSet
  # Required when hostNetwork: true — resolves K8s service DNS in host net mode
  dnsPolicy: ClusterFirstWithHostNet

# Rolling update strategy — required when hostNetwork: true
# Two pods cannot bind the same host port, so maxUnavailable must be > 0
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 0

# Bind directly to node network — ports 80/443 available as-is
hostNetwork: true

# Run on ALL nodes including control plane (required for EIP failover)
tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

# Entry points (bound to host network — port IS the host port)
ports:
  web:
    port: 80
    expose:
      default: true
    exposedPort: 80
    protocol: TCP
  websecure:
    port: 443
    expose:
      default: true
    exposedPort: 443
    protocol: TCP
    # TLS terminated at CloudFront, not Traefik — no TLS config needed here
  metrics:
    port: 9100
    expose:
      default: false
    exposedPort: 9100
    protocol: TCP

# Disable LoadBalancer service (using hostNetwork + EIP instead)
service:
  enabled: false

# Prometheus metrics
metrics:
  prometheus:
    entryPoint: metrics
    addEntryPointsLabels: true
    addRoutersLabels: true
    addServicesLabels: true

# OTLP tracing → Tempo (in-cluster)
tracing:
  otlp:
    grpc:
      endpoint: "tempo.monitoring.svc.cluster.local:4317"
      insecure: true

# Additional CLI arguments
additionalArguments:
  - "--tracing.otlp.grpc.endpoint=tempo.monitoring.svc.cluster.local:4317"
  - "--tracing.otlp.grpc.insecure=true"
  - "--metrics.prometheus=true"
  - "--metrics.prometheus.entryPoint=metrics"
  - "--metrics.prometheus.addEntryPointsLabels=true"
  - "--metrics.prometheus.addRoutersLabels=true"
  - "--metrics.prometheus.addServicesLabels=true"

# Logging
logs:
  general:
    level: INFO
  access:
    enabled: true

# Security context — allow binding to privileged ports 80/443
# The chart defaults to non-root (uid 65532) with all capabilities dropped,
# which prevents binding to ports < 1024. We add NET_BIND_SERVICE.
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
    add: [NET_BIND_SERVICE]
  readOnlyRootFilesystem: true

podSecurityContext:
  runAsGroup: 0
  runAsNonRoot: false
  runAsUser: 0
