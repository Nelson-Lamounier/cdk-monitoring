# @format
# @description Reusable Kubernetes Deployment Workflow (Centralized)
#
# Infra-only pipeline for the K8s (kubeadm) infrastructure.
# Bootstrap/app manifests synced by independent S3 pipelines.
#
# 9-Stack Deployment Order:
#   1.  Data             (DynamoDB + S3 + SSM Parameters)
#   2.  Base             (VPC networking + SG + KMS + EBS + EIP + S3 Scripts)
#   2b. Sync             (Seed boot scripts to S3 — Day-1 safety)
#   2c. Golden AMI       (Bake containerd + kubeadm into AMI)
#  2d.  SSM Automation   (Bootstrap orchestration documents)
#   3.  Compute          (Control plane EC2 + ASG + SSM Docs)
#   3b. AppWorker        (Application worker node)
#   3c. MonitoringWorker (Monitoring worker node)
#   4.  AppIam           (Application-tier IAM grants)
#   5.  Api              (API Gateway + Lambda)
#   6.  Edge             (CloudFront + ACM + WAF in us-east-1)
#
# Post-Deploy (manifests managed by ArgoCD GitOps):
#   7.  Sync Static Assets          (S3 + CloudFront invalidation)
#   8.  ArgoCD Health Gate          (wait for GitOps sync)
#   9.  Verify & Smoke Tests
#  10.  Deployment Failure Alert    (halt + preserve + alert)
#  11.  Summary
#
# Concurrency:
#   This is a workflow_call (reusable) workflow — concurrency MUST be set
#   on the caller (e.g., deploy-kubernetes-dev.yml, deploy-kubernetes-prod.yml).
#   Each caller defines: concurrency: { group: deploy-k8s-<env>, cancel-in-progress: false }

name: Deploy K8s (Reusable)

on:
  workflow_call:
    inputs:
      environment:
        description: "Target environment (development, staging, production)"
        required: true
        type: string
      cdk-environment:
        description: "CDK environment name"
        required: true
        type: string
      require-approval:
        description: "CDK deployment approval mode"
        required: false
        default: "never"
        type: string
      enable-security-scan:
        description: "Run IaC security scan"
        required: false
        default: false
        type: boolean
      security-scan-blocking:
        description: "Fail pipeline on security scan findings"
        required: false
        default: false
        type: boolean
      skip-verification:
        description: "Skip post-deployment verification"
        required: false
        default: false
        type: boolean
      domain-name:
        description: "Domain name for CloudFront Edge stack (optional — falls back to vars.DOMAIN_NAME)"
        required: false
        type: string
        default: ""
      hosted-zone-id:
        description: "Route53 Hosted Zone ID (optional — falls back to vars.HOSTED_ZONE_ID)"
        required: false
        type: string
        default: ""
      cross-account-role-arn:
        description: "Cross-account IAM role for Route53 (optional — falls back to vars.DNS_VALIDATION_ROLE)"
        required: false
        type: string
        default: ""
      enable-golden-ami-build:
        description: "Trigger Golden AMI build after Data stack deploys"
        required: false
        default: true
        type: boolean
    secrets:
      AWS_OIDC_ROLE:
        description: "AWS OIDC role ARN"
        required: false
      VERIFICATION_SECRET:
        required: false

env:
  NODE_VERSION: "22"
  AWS_REGION: ${{ vars.AWS_REGION || 'eu-west-1' }}
  DEPLOY_ENVIRONMENT: ${{ inputs.environment }}

jobs:
  # ===========================================================================
  # Setup, Build & Synthesize
  # ===========================================================================
  setup:
    name: Setup & Synthesize
    runs-on: ubuntu-latest
    timeout-minutes: 20
    environment: ${{ inputs.environment }}
    outputs:
      aws-account-id: ${{ steps.account-id.outputs.account_id }}
      commit-sha: ${{ github.sha }}
      commit-short-sha: ${{ steps.commit-info.outputs.short_sha }}
      node-version: ${{ steps.setup-tools.outputs.node-version }}
      # Resolved edge configuration
      domain-name: ${{ steps.resolve-edge.outputs.domain-name }}
      hosted-zone-id: ${{ steps.resolve-edge.outputs.hosted-zone-id }}
      cross-account-role-arn: ${{ steps.resolve-edge.outputs.cross-account-role-arn }}
      # Stack names from synthesize-ci.ts (8-stack architecture)
      data-stack: ${{ steps.synth.outputs.data }}
      base-stack: ${{ steps.synth.outputs.base }}
      controlplane-stack: ${{ steps.synth.outputs.controlPlane }}
      ssmautomation-stack: ${{ steps.synth.outputs.ssmAutomation }}
      worker-stack: ${{ steps.synth.outputs.appWorker }}
      monitoring-worker-stack: ${{ steps.synth.outputs.monitoringWorker }}
      appiam-stack: ${{ steps.synth.outputs.appIam }}
      edge-stack: ${{ steps.synth.outputs.edge }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Record Commit Information
        id: commit-info
        run: echo "short_sha=${GITHUB_SHA::8}" >> $GITHUB_OUTPUT

      - name: Setup Node.js and Yarn
        id: setup-tools
        uses: ./.github/actions/setup-node-yarn
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Validate AWS Account ID
        id: account-id
        run: |
          ACCOUNT_ID="${{ vars.AWS_ACCOUNT_ID }}"
          if [ -z "$ACCOUNT_ID" ]; then
            echo "ERROR: AWS_ACCOUNT_ID variable not configured"
            exit 1
          fi
          if ! [[ "$ACCOUNT_ID" =~ ^[0-9]{12}$ ]]; then
            echo "ERROR: Invalid AWS account ID format"
            exit 1
          fi
          echo "::add-mask::$ACCOUNT_ID"
          echo "account_id=$ACCOUNT_ID" >> $GITHUB_OUTPUT

      - name: Build TypeScript
        run: just build

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: Resolve Edge Configuration
        id: resolve-edge
        run: |
          # Domain: input > vars.DOMAIN_NAME
          DOMAIN="${{ inputs.domain-name }}"
          if [ -z "$DOMAIN" ]; then DOMAIN="${{ vars.DOMAIN_NAME }}"; fi
          if [ -z "$DOMAIN" ]; then
            echo "ERROR: No domain name. Set DOMAIN_NAME in environment '${{ inputs.environment }}'"
            exit 1
          fi

          # Hosted Zone: input > vars.HOSTED_ZONE_ID
          HZ_ID="${{ inputs.hosted-zone-id }}"
          if [ -z "$HZ_ID" ]; then HZ_ID="${{ vars.HOSTED_ZONE_ID }}"; fi
          if [ -z "$HZ_ID" ]; then
            echo "ERROR: No hosted zone ID. Set HOSTED_ZONE_ID in environment '${{ inputs.environment }}'"
            exit 1
          fi

          # Cross-account role: input > vars.DNS_VALIDATION_ROLE
          ROLE_ARN="${{ inputs.cross-account-role-arn }}"
          if [ -z "$ROLE_ARN" ]; then ROLE_ARN="${{ vars.DNS_VALIDATION_ROLE }}"; fi
          if [ -z "$ROLE_ARN" ]; then
            echo "ERROR: No cross-account role. Set DNS_VALIDATION_ROLE in environment '${{ inputs.environment }}'"
            exit 1
          fi

          echo "domain-name=$DOMAIN" >> $GITHUB_OUTPUT
          echo "hosted-zone-id=$HZ_ID" >> $GITHUB_OUTPUT
          echo "cross-account-role-arn=$ROLE_ARN" >> $GITHUB_OUTPUT
          echo "✓ Edge config: $DOMAIN (Zone: $HZ_ID)"

      - name: Synthesize & Determine Stack Names
        id: synth
        env:
          DOMAIN_NAME: ${{ steps.resolve-edge.outputs.domain-name }}
          HOSTED_ZONE_ID: ${{ steps.resolve-edge.outputs.hosted-zone-id }}
          CROSS_ACCOUNT_ROLE_ARN: ${{ steps.resolve-edge.outputs.cross-account-role-arn }}
          NOTIFICATION_EMAIL: ${{ vars.NOTIFICATION_EMAIL }}
          SES_FROM_EMAIL: ${{ vars.SES_FROM_EMAIL }}
          VERIFICATION_SECRET: ${{ secrets.VERIFICATION_SECRET }}
          VERIFICATION_BASE_URL: ${{ vars.VERIFICATION_BASE_URL }}
        run: just ci-synth kubernetes ${{ inputs.cdk-environment }}

      - name: Upload Synthesized Templates
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: cdk-out-kubernetes-${{ inputs.cdk-environment }}-${{ steps.commit-info.outputs.short_sha }}
          path: infra/cdk.out/
          retention-days: 30
          if-no-files-found: error

      - name: Upload for Security Scan
        if: inputs.enable-security-scan
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: cdk-synthesis
          path: infra/cdk.out/
          retention-days: 7
          if-no-files-found: error

  # ===========================================================================
  # IaC Security Scan (Conditional)
  # ===========================================================================
  security-scan:
    name: Security Scan
    needs: setup
    if: inputs.enable-security-scan
    uses: ./.github/workflows/_iac-security-scan.yml
    with:
      environment: ${{ inputs.environment }}
      enforce-blocking: ${{ inputs.security-scan-blocking }}
      cdk-output-path: "cdk.out"
      skip-checks: ""
      soft-fail-on: ${{ inputs.security-scan-blocking && '' || 'LOW' }}
    secrets: inherit

  # ===========================================================================
  # Drift Detection (staging/production — informational)
  # ===========================================================================
  drift-detection:
    name: Drift Detection
    needs: [setup]
    runs-on: ubuntu-latest
    timeout-minutes: 15
    environment: ${{ inputs.environment }}
    if: needs.setup.result == 'success' && inputs.cdk-environment != 'development'

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Setup Node.js and Yarn
        uses: ./.github/actions/setup-node-yarn
        with:
          node-version: ${{ needs.setup.outputs.node-version }}

      - name: Install Dependencies
        run: yarn install --frozen-lockfile
        working-directory: infra

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: Run Drift Detection
        run: just ci-drift kubernetes ${{ inputs.cdk-environment }} --region ${{ env.AWS_REGION }}
        env:
          AWS_ACCOUNT_ID: ${{ vars.AWS_ACCOUNT_ID }}

  # ===========================================================================
  # Template Validation (cfn-lint)
  # ===========================================================================
  validate-templates:
    name: Validate Templates
    needs: [setup]
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout Config
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          sparse-checkout: .cfnlintrc
          sparse-checkout-cone-mode: false

      - name: Download Synthesized Templates
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093 # v4.3.0
        with:
          name: cdk-out-kubernetes-${{ inputs.cdk-environment }}-${{ needs.setup.outputs.commit-short-sha }}
          path: cdk.out/

      - name: Setup Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: "3.11"

      - name: Install cfn-lint
        run: pip install cfn-lint

      - name: Validate CloudFormation Templates
        shell: python3 {0}
        run: |
          import sys, glob
          sys.setrecursionlimit(10000)
          from cfnlint import decode, runner
          from cfnlint.config import ConfigMixIn

          templates = sorted(glob.glob("cdk.out/**/*.template.json", recursive=True))
          print(f"Validating {len(templates)} templates...")
          total_errors = 0
          for tpl in templates:
              print(f"\n--- {tpl} ---")
              try:
                  config = ConfigMixIn(["--template", tpl])
                  matches = list(runner.Runner(config).run())
                  if matches:
                      total_errors += len(matches)
                      for match in matches:
                          print(f"  {match}")
                  else:
                      print("  PASS")
              except RecursionError:
                  print(f"  SKIP (template too deeply nested)")
              except Exception as e:
                  print(f"  SKIP ({type(e).__name__}: {e})")

          if total_errors > 0:
              print(f"\n{total_errors} validation errors found")
              sys.exit(1)
          else:
              print(f"\nAll {len(templates)} templates passed validation")

  # ===========================================================================
  # Deploy Data Stack (1/6) — DynamoDB + S3 + SSM Parameters
  # ===========================================================================
  deploy-data:
    name: Deploy Data
    needs: [setup, validate-templates, security-scan, drift-detection]
    if: >-
      always()
      && needs.setup.result == 'success'
      && needs.validate-templates.result == 'success'
      && (needs.security-scan.result == 'success' || needs.security-scan.result == 'skipped')
      && (needs.drift-detection.result == 'success' || needs.drift-detection.result == 'skipped')
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.data-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Build Golden AMI (2c — BEFORE Compute)
  #
  # Triggers Image Builder to bake containerd + kubeadm into a Golden AMI.
  # Runs AFTER S3 bootstrap sync so the boot script is available, but
  # BEFORE Compute so the control plane launches from the baked AMI.
  # On Day-0: AMI matches parent → build triggered (first bake).
  # On Day-1+: AMI differs from parent → skipped (already baked).
  # ===========================================================================
  build-golden-ami:
    name: Build Golden AMI
    needs: [setup, sync-bootstrap-content]
    if: >-
      always()
      && needs.sync-bootstrap-content.result == 'success'
      && inputs.enable-golden-ami-build
    uses: ./.github/workflows/_build-golden-ami.yml
    with:
      environment: ${{ inputs.environment }}
      cdk-environment: ${{ inputs.cdk-environment }}
    secrets:
      AWS_OIDC_ROLE: ${{ secrets.AWS_OIDC_ROLE }}

  # ===========================================================================
  # Deploy Base Stack (2/6) — VPC networking + SG + KMS + EBS + EIP
  # ===========================================================================
  deploy-base:
    name: Deploy Base
    needs: [setup, deploy-data]
    if: always() && needs.deploy-data.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.base-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Sync K8s Boot Scripts to S3 (2b/6) — Day-1 Safety
  #
  # Syncs kubernetes-app/k8s-bootstrap/ scripts into the S3 bucket created by BaseStack.
  # The control plane downloads boot-k8s.sh from s3://{bucket}/k8s-bootstrap/boot/boot-k8s.sh
  # at launch. Runs BEFORE Compute so the script is waiting in S3
  # by the time the EC2 instance launches — zero race condition.
  # ===========================================================================
  sync-bootstrap-content:
    name: Sync K8s Boot Scripts to S3
    needs: [setup, deploy-base]
    if: always() && needs.deploy-base.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    environment: ${{ inputs.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
          audience: sts.amazonaws.com

      - name: Sync Bootstrap Scripts to S3
        run: |
          SSM_PREFIX="/k8s/${{ inputs.cdk-environment }}"
          REGION="${{ vars.AWS_REGION || 'eu-west-1' }}"

          # Resolve scripts bucket from SSM (created by BaseStack)
          BUCKET=$(aws ssm get-parameter \
            --name "${SSM_PREFIX}/scripts-bucket" \
            --query 'Parameter.Value' --output text \
            --region "${REGION}" 2>/dev/null || echo "")

          if [ -z "$BUCKET" ]; then
            echo "ERROR: Could not resolve scripts bucket from SSM parameter ${SSM_PREFIX}/scripts-bucket"
            echo "Ensure the Base stack deployed successfully and published the parameter."
            exit 1
          fi
          echo "Resolved scripts bucket: ${BUCKET}"

          # Verify source directory exists
          SOURCE_DIR="./kubernetes-app/k8s-bootstrap"
          if [ ! -d "$SOURCE_DIR" ]; then
            echo "ERROR: Source directory not found: ${SOURCE_DIR}"
            exit 1
          fi

          # Count files before sync
          FILE_COUNT=$(find "$SOURCE_DIR" -type f | wc -l | tr -d ' ')
          echo "Syncing ${FILE_COUNT} files from ${SOURCE_DIR} to s3://${BUCKET}/k8s-bootstrap/"

          # Sync with --delete to remove stale files
          aws s3 sync "$SOURCE_DIR" "s3://${BUCKET}/k8s-bootstrap/" \
            --delete --region "${REGION}"

          echo "✓ ${FILE_COUNT} bootstrap files seeded — EC2 instances can now boot safely"

  # ===========================================================================
  # Deploy SSM Automation Stack — Bootstrap Orchestration Documents
  #
  # Must deploy before Compute so SSM Automation documents exist when
  # EC2 user data tries to trigger them.
  # ===========================================================================
  deploy-ssm-automation:
    name: Deploy SSM Automation
    needs: [setup, deploy-base]
    if: always() && needs.deploy-base.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.ssmautomation-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Deploy Compute Stack (3/8) — Control Plane EC2 + ASG + SSM Docs
  #
  # Waits for Golden AMI bake (or skip) AND SSM Automation so the
  # Launch Template resolves to the latest baked AMI and the bootstrap
  # documents exist when user data triggers them.
  # ===========================================================================
  deploy-controlplane:
    name: Deploy Control Plane
    needs:
      [
        setup,
        deploy-base,
        sync-bootstrap-content,
        build-golden-ami,
        deploy-ssm-automation,
      ]
    if: >-
      always()
      && needs.sync-bootstrap-content.result == 'success'
      && (needs.build-golden-ami.result == 'success' || needs.build-golden-ami.result == 'skipped')
      && needs.deploy-ssm-automation.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.controlplane-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Deploy App Worker Stack (3b/8) — Application Worker Node
  # ===========================================================================
  deploy-app-worker:
    name: Deploy App Worker
    needs: [setup, deploy-controlplane]
    if: always() && needs.deploy-controlplane.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.worker-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Deploy Monitoring Worker Stack (3c/8) — Monitoring Worker Node
  # ===========================================================================
  deploy-monitoring-worker:
    name: Deploy Monitoring Worker
    needs: [setup, deploy-controlplane]
    if: always() && needs.deploy-controlplane.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.monitoring-worker-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Watch SSM Bootstrap — Monitor SSM Automation Execution
  #
  # After Compute + Worker stacks deploy (which trigger SSM Automation
  # via user data), poll SSM Automation execution status and display
  # step-level progress in GitHub Actions logs.
  # ===========================================================================
  watch-ssm-bootstrap:
    name: Watch SSM Bootstrap
    needs:
      [setup, deploy-controlplane, deploy-app-worker, deploy-monitoring-worker]
    if: >-
      always()
      && needs.deploy-controlplane.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    environment: ${{ inputs.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
          audience: sts.amazonaws.com

      - name: Watch SSM Bootstrap Progress
        env:
          SSM_PREFIX: /k8s/${{ inputs.environment }}
          REGION: ${{ vars.AWS_REGION || 'eu-west-1' }}
        run: |
          echo "=== Watching SSM Automation Bootstrap ==="

          # Read execution ID from SSM (set by EC2 user data)
          MAX_WAIT=120
          WAITED=0
          EXECUTION_ID=""

          while [ -z "$EXECUTION_ID" ] && [ $WAITED -lt $MAX_WAIT ]; do
            EXECUTION_ID=$(aws ssm get-parameter \
              --name "${SSM_PREFIX}/bootstrap/execution-id" \
              --query "Parameter.Value" --output text \
              --region "$REGION" 2>/dev/null || echo "")
            if [ "$EXECUTION_ID" = "None" ] || [ -z "$EXECUTION_ID" ]; then
              EXECUTION_ID=""
              echo "Waiting for SSM Automation execution ID... (${WAITED}s / ${MAX_WAIT}s)"
              sleep 10
              WAITED=$((WAITED + 10))
            fi
          done

          if [ -z "$EXECUTION_ID" ]; then
            echo "::warning::SSM Automation execution ID not found after ${MAX_WAIT}s — bootstrap may not have started"
            exit 0
          fi

          echo "Tracking execution: $EXECUTION_ID"

          # Poll until completion
          while true; do
            EXECUTION=$(aws ssm get-automation-execution \
              --automation-execution-id "$EXECUTION_ID" \
              --region "$REGION" 2>/dev/null)

            STATUS=$(echo "$EXECUTION" | jq -r '.AutomationExecution.AutomationExecutionStatus')

            echo "::group::SSM Bootstrap Steps (Status: $STATUS)"
            echo "$EXECUTION" | jq -r '.AutomationExecution.StepExecutions[] | "\(.StepName): \(.StepStatus) (\(.ExecutionStartDateTime // "pending") → \(.ExecutionEndDateTime // "running"))"' 2>/dev/null || true
            echo "::endgroup::"

            case "$STATUS" in
              Success)
                echo "✅ SSM Automation completed successfully"
                break
                ;;
              Failed|Cancelled|TimedOut)
                echo "❌ SSM Automation $STATUS"
                echo "$EXECUTION" | jq -r '.AutomationExecution.StepExecutions[] | select(.StepStatus == "Failed") | "FAILED STEP: \(.StepName)\nReason: \(.FailureMessage // "unknown")"' 2>/dev/null || true
                exit 1
                ;;
              *)
                sleep 15
                ;;
            esac
          done

  # ===========================================================================
  # Monitor Worker Join (real-time observability)
  #
  # Runs in parallel with worker deployments. Polls CloudFormation stack
  # events every 30s to show ASG status and cfn-signal progress.
  # Observational only — does not block the pipeline.
  # ===========================================================================
  monitor-worker-join:
    name: Monitor Worker Join
    needs: [setup, deploy-controlplane]
    if: always() && needs.deploy-controlplane.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    environment: ${{ inputs.environment }}
    continue-on-error: true

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
          audience: sts.amazonaws.com

      - name: Monitor Worker Stack Progress
        env:
          APP_WORKER_STACK: ${{ needs.setup.outputs.worker-stack }}
          MON_WORKER_STACK: ${{ needs.setup.outputs.monitoring-worker-stack }}
          AWS_REGION: ${{ vars.AWS_REGION || 'eu-west-1' }}
        run: |
          POLL_INTERVAL=30
          MAX_POLLS=50  # 25 minutes max monitoring window
          STALE_THRESHOLD=5  # Mark stack as NOT STARTED after 5 consecutive PENDING polls (2.5 min)
          STACKS=("$APP_WORKER_STACK" "$MON_WORKER_STACK")

          echo "## Worker Join Monitor"
          echo ""
          echo "Monitoring stacks:"
          echo "  - App Worker:        $APP_WORKER_STACK"
          echo "  - Monitoring Worker: $MON_WORKER_STACK"
          echo "  - Poll interval:     ${POLL_INTERVAL}s"
          echo "  - Max duration:      $(( MAX_POLLS * POLL_INTERVAL ))s"
          echo "  - Stale threshold:   ${STALE_THRESHOLD} polls ($(( STALE_THRESHOLD * POLL_INTERVAL ))s)"
          echo ""
          echo "---"

          # Track completion and stale counters per stack
          declare -A STACK_DONE
          declare -A STACK_STALE_COUNT
          declare -A STACK_LAST_STATUS
          for S in "${STACKS[@]}"; do
            STACK_DONE[$S]=false
            STACK_STALE_COUNT[$S]=0
            STACK_LAST_STATUS[$S]=""
          done

          for POLL in $(seq 1 $MAX_POLLS); do
            TIMESTAMP=$(date -u +"%H:%M:%S")
            ALL_DONE=true

            for STACK_NAME in "${STACKS[@]}"; do
              # Skip if already completed
              if [ "${STACK_DONE[$STACK_NAME]}" = "true" ]; then
                continue
              fi

              # Short label for output
              LABEL=$(echo "$STACK_NAME" | grep -oE '(App|Monitoring)Worker' || echo "$STACK_NAME")

              # Get latest stack events (last 5)
              EVENTS=$(aws cloudformation describe-stack-events \
                --stack-name "$STACK_NAME" \
                --max-items 5 \
                --query "StackEvents[].{Type:ResourceType,LogId:LogicalResourceId,Status:ResourceStatus,Reason:ResourceStatusReason}" \
                --output json \
                --region "$AWS_REGION" 2>/dev/null || echo "[]")

              # Extract ASG status specifically
              ASG_STATUS=$(echo "$EVENTS" | jq -r '
                [.[] | select(.Type == "AWS::AutoScaling::AutoScalingGroup")] |
                first // empty |
                "\(.Status) | \(.Reason // "—")"' 2>/dev/null || echo "—")

              # Get overall stack status
              STACK_STATUS=$(aws cloudformation describe-stacks \
                --stack-name "$STACK_NAME" \
                --query "Stacks[0].StackStatus" \
                --output text \
                --region "$AWS_REGION" 2>/dev/null || echo "PENDING")

              # ---------------------------------------------------------------
              # Stale detection: if the stack stays in a non-progress state
              # (e.g. PENDING, CREATE_COMPLETE from a previous run) for too
              # many consecutive polls, the deploy job likely failed at CI
              # level and CloudFormation was never invoked.
              # ---------------------------------------------------------------
              case "$STACK_STATUS" in
                *_IN_PROGRESS)
                  # Stack is actively updating — reset stale counter
                  STACK_STALE_COUNT[$STACK_NAME]=0
                  ;;
                PENDING)
                  # Stack not found or API error — increment stale counter
                  STACK_STALE_COUNT[$STACK_NAME]=$(( ${STACK_STALE_COUNT[$STACK_NAME]} + 1 ))
                  ;;
                *)
                  # Terminal state from a PREVIOUS deployment (no new update started)
                  if [ "${STACK_LAST_STATUS[$STACK_NAME]}" = "$STACK_STATUS" ]; then
                    STACK_STALE_COUNT[$STACK_NAME]=$(( ${STACK_STALE_COUNT[$STACK_NAME]} + 1 ))
                  else
                    STACK_STALE_COUNT[$STACK_NAME]=0
                  fi
                  ;;
              esac
              STACK_LAST_STATUS[$STACK_NAME]="$STACK_STATUS"

              # Check if stack is stale (deploy job never started CloudFormation)
              if [ "${STACK_STALE_COUNT[$STACK_NAME]}" -ge "$STALE_THRESHOLD" ]; then
                STACK_DONE[$STACK_NAME]=true
                echo "[$TIMESTAMP] $LABEL | ⚠️  NOT STARTED — no CloudFormation activity after ${STALE_THRESHOLD} polls"
                echo "[$TIMESTAMP] $LABEL |    Deploy job may have failed at CI level before reaching CDK deploy"
                continue
              fi

              # Check for signal-related events
              SIGNAL_INFO=$(echo "$EVENTS" | jq -r '
                [.[] | select(.Reason != null and (.Reason | test("signal|Signal"; "i")))] |
                first // empty |
                .Reason // empty' 2>/dev/null || echo "")

              # Format output
              if [ -n "$SIGNAL_INFO" ]; then
                echo "[$TIMESTAMP] $LABEL | $STACK_STATUS | ASG: $ASG_STATUS | Signal: $SIGNAL_INFO"
              else
                echo "[$TIMESTAMP] $LABEL | $STACK_STATUS"
              fi

              # Check if stack is terminal
              case "$STACK_STATUS" in
                *COMPLETE|*FAILED|*ROLLBACK_COMPLETE)
                  STACK_DONE[$STACK_NAME]=true
                  if [[ "$STACK_STATUS" == *"COMPLETE" ]] && [[ "$STACK_STATUS" != *"ROLLBACK"* ]]; then
                    echo "[$TIMESTAMP] $LABEL | ✅ Stack update completed successfully"
                  else
                    echo "[$TIMESTAMP] $LABEL | ❌ Stack update failed: $STACK_STATUS"
                  fi
                  ;;
                *)
                  ALL_DONE=false
                  ;;
              esac
            done

            if [ "$ALL_DONE" = "true" ]; then
              echo ""
              echo "---"
              echo "All worker stacks reached terminal state."
              break
            fi

            sleep $POLL_INTERVAL
          done

          # Final summary
          echo ""
          echo "## Summary"
          echo ""
          echo "| Stack | Final Status |"
          echo "|-------|-------------|"
          for STACK_NAME in "${STACKS[@]}"; do
            LABEL=$(echo "$STACK_NAME" | grep -oE '(App|Monitoring)Worker' || echo "$STACK_NAME")
            if [ "${STACK_STALE_COUNT[$STACK_NAME]}" -ge "$STALE_THRESHOLD" ]; then
              echo "| $LABEL | ⚠️ NOT STARTED (deploy job failed at CI level) |"
            else
              FINAL=$(aws cloudformation describe-stacks \
                --stack-name "$STACK_NAME" \
                --query "Stacks[0].StackStatus" \
                --output text \
                --region "$AWS_REGION" 2>/dev/null || echo "UNKNOWN")
              echo "| $LABEL | $FINAL |"
            fi
          done

  # ===========================================================================
  # Deploy AppIam Stack (4/8) — Application-tier IAM grants
  # ===========================================================================
  deploy-appiam:
    name: Deploy App IAM
    needs: [setup, deploy-app-worker, deploy-monitoring-worker]
    if: always() && needs.deploy-app-worker.result == 'success' && needs.deploy-monitoring-worker.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.appiam-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
    secrets: inherit

  # ===========================================================================
  # Deploy Edge Stack (5/6) — CloudFront + ACM + WAF (us-east-1)
  # Origin: Elastic IP → Traefik
  # ===========================================================================
  deploy-edge:
    name: Deploy Edge
    needs: [setup, deploy-appiam, deploy-data]
    if: always() && needs.deploy-appiam.result == 'success' && needs.deploy-data.result == 'success'
    uses: ./.github/workflows/_deploy-stack.yml
    with:
      stack-name: ${{ needs.setup.outputs.edge-stack }}
      project: kubernetes
      environment: ${{ inputs.environment }}
      aws-account-id: ${{ needs.setup.outputs.aws-account-id }}
      aws-region: ${{ vars.ROUTE53_REGION || 'us-east-1' }}
      require-approval: ${{ inputs.require-approval }}
      outputs-directory: "deployment-outputs"
      domain-name: ${{ needs.setup.outputs.domain-name }}
      hosted-zone-id: ${{ needs.setup.outputs.hosted-zone-id }}
      cross-account-role-arn: ${{ needs.setup.outputs.cross-account-role-arn }}
    secrets: inherit

  # ===========================================================================
  # K8s Manifest Deployment — ArgoCD (GitOps)
  #
  # Monitoring and application manifests are deployed by ArgoCD running
  # on the kubeadm node. ArgoCD watches this Git repo and auto-syncs changes
  # within ~3 minutes. No CI pipeline jobs needed.
  #
  # See: kubernetes-app/k8s-bootstrap/system/argocd/ for ArgoCD config and Application CRDs.
  # SSM Run Command documents are retained as a fallback for secret updates.
  # ===========================================================================

  # ===========================================================================
  # Sync Static Assets to S3 + CloudFront Cache Invalidation
  #
  # Stays in Pipeline A: .next/static/ is a BUILD ARTIFACT extracted from
  # the Docker image during CDK synth. It only exists in the cdk-out artifact,
  # which Pipeline B does not produce.
  # ===========================================================================
  sync-static-assets:
    name: Sync Static Assets
    needs: [setup, deploy-data, deploy-edge]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    environment: ${{ inputs.environment }}
    if: always() && needs.deploy-data.result == 'success' && (needs.deploy-edge.result == 'success' || needs.deploy-edge.result == 'skipped')
    outputs:
      s3-bucket: ${{ steps.sync.outputs.bucket }}
      files-synced: ${{ steps.sync.outputs.files_synced }}
      invalidation-id: ${{ steps.sync.outputs.invalidation_id }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Setup Node.js and Yarn
        uses: ./.github/actions/setup-node-yarn
        with:
          node-version: ${{ needs.setup.outputs.node-version }}

      - name: Install Dependencies
        run: yarn install --frozen-lockfile
        working-directory: infra

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: Download Build Artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093 # v4.3.0
        with:
          name: cdk-out-kubernetes-${{ inputs.cdk-environment }}-${{ needs.setup.outputs.commit-short-sha }}
          path: cdk.out/

      - name: Sync Assets & Invalidate Cache
        id: sync
        run: |
          SYNC_CMD="just ci-sync-assets ${{ inputs.cdk-environment }}"
          SYNC_CMD="$SYNC_CMD --region ${{ env.AWS_REGION }}"

          if [ -n "${{ needs.setup.outputs.domain-name }}" ]; then
            SYNC_CMD="$SYNC_CMD --domain ${{ needs.setup.outputs.domain-name }}"
          fi

          eval $SYNC_CMD

  # ===========================================================================
  # Verify & Smoke Tests
  # ===========================================================================
  verify-and-test:
    name: Verify & Smoke Tests
    needs:
      [
        setup,
        deploy-controlplane,
        deploy-app-worker,
        deploy-monitoring-worker,
        deploy-edge,
        sync-static-assets,
      ]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    environment: ${{ inputs.environment }}
    if: >-
      always()
      && needs.deploy-controlplane.result == 'success'
      && needs.deploy-app-worker.result == 'success'
      && needs.deploy-monitoring-worker.result == 'success'
      && inputs.skip-verification != true
    outputs:
      status: ${{ steps.smoke.outputs.status }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Setup Node.js and Yarn
        uses: ./.github/actions/setup-node-yarn
        with:
          node-version: ${{ needs.setup.outputs.node-version }}

      - name: Install Dependencies
        run: yarn install --frozen-lockfile
        working-directory: infra

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: Wait for ArgoCD Token
        id: argocd-token
        continue-on-error: true # Day-0: token may not exist on first deployment
        run: |
          SECRET_NAME="k8s/${{ inputs.cdk-environment }}/argocd-ci-token"
          MAX_ATTEMPTS=20
          INTERVAL=30

          echo "Polling Secrets Manager for ArgoCD CI token..."
          echo "  Secret: ${SECRET_NAME}"
          echo "  Max wait: $(( MAX_ATTEMPTS * INTERVAL ))s"
          echo ""

          for i in $(seq 1 $MAX_ATTEMPTS); do
            TOKEN=$(aws secretsmanager get-secret-value \
              --secret-id "$SECRET_NAME" \
              --query SecretString --output text 2>/dev/null || echo "")

            if [ -n "$TOKEN" ]; then
              echo "::add-mask::$TOKEN"
              echo "token=$TOKEN" >> $GITHUB_OUTPUT
              echo "✓ ArgoCD token retrieved (attempt $i/${MAX_ATTEMPTS})"
              exit 0
            fi

            echo "Attempt $i/${MAX_ATTEMPTS} — token not ready, waiting ${INTERVAL}s..."
            sleep $INTERVAL
          done

          echo "::warning::ArgoCD token not found after ${MAX_ATTEMPTS} attempts — Day-0 bootstrap; skipping ArgoCD verification"

      # -----------------------------------------------------------------------
      # ArgoCD Server Health (Infrastructure Check)
      #
      # Verifies the ArgoCD SERVER is reachable — this is an infra concern.
      # Per-application health (monitoring, nextjs, traefik) is verified
      # by Pipeline B (gitops-k8s-dev.yml → verify-argocd job).
      #
      # On Day-0 (no token), this step is skipped gracefully.
      # -----------------------------------------------------------------------
      - name: "ArgoCD Server Health"
        if: steps.argocd-token.outputs.token != '' && steps.argocd-token.outcome == 'success'
        env:
          ARGOCD_TOKEN: ${{ steps.argocd-token.outputs.token }}
        run: |
          # Discover ArgoCD server via EIP from SSM
          SSM_PREFIX="/k8s/${{ inputs.cdk-environment }}"
          EIP=$(aws ssm get-parameter \
            --name "${SSM_PREFIX}/elastic-ip" \
            --query 'Parameter.Value' --output text 2>/dev/null || echo "")

          if [ -z "$EIP" ]; then
            echo "::error::Could not resolve EIP from SSM — cannot verify ArgoCD"
            exit 1
          fi

          ARGOCD_SERVER="https://${EIP}/argocd"
          echo "ArgoCD server: ${ARGOCD_SERVER}"

          # Verify ArgoCD API is reachable (infra check — is the pod alive?)
          MAX_POLL=6
          POLL_INTERVAL=20
          echo "Verifying ArgoCD server is reachable (timeout: $(( MAX_POLL * POLL_INTERVAL ))s)..."

          for ATTEMPT in $(seq 1 $MAX_POLL); do
            HTTP_CODE=$(curl -sk -o /dev/null -w '%{http_code}' --max-time 10 \
              -H "Authorization: Bearer ${ARGOCD_TOKEN}" \
              "${ARGOCD_SERVER}/api/v1/applications" 2>/dev/null || echo "000")

            if [ "$HTTP_CODE" = "200" ]; then
              echo "✅ ArgoCD server is reachable (HTTP ${HTTP_CODE})"
              echo "   Per-application health will be verified by Pipeline B (GitOps pipeline)."
              exit 0
            fi

            echo "  Attempt $ATTEMPT/$MAX_POLL — HTTP ${HTTP_CODE}, retrying in ${POLL_INTERVAL}s..."
            sleep $POLL_INTERVAL
          done

          echo "::error::ArgoCD server unreachable after $(( MAX_POLL * POLL_INTERVAL ))s — the ArgoCD seed may have failed to install"
          exit 1

      - name: Run K8s App Smoke Tests
        id: smoke
        run: |
          SMOKE_CMD="just ci-smoke-kubernetes-infra ${{ inputs.cdk-environment }} --region ${{ env.AWS_REGION }}"

          if [ -n "${{ needs.setup.outputs.domain-name }}" ]; then
            SMOKE_CMD="$SMOKE_CMD --cloudfront-domain ${{ needs.setup.outputs.domain-name }}"
          fi

          eval $SMOKE_CMD

      # -----------------------------------------------------------------------
      # Failure Diagnostics — extract CFn events + K8s state on failure
      # -----------------------------------------------------------------------
      - name: "Capture Failure Diagnostics"
        if: failure()
        continue-on-error: true
        run: |
          echo "## Failure Diagnostics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment**: ${{ inputs.environment }} | **Commit**: \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # CloudFormation failed events for each stack
          STACKS=(
            "${{ needs.setup.outputs.data-stack }}"
            "${{ needs.setup.outputs.base-stack }}"
            "${{ needs.setup.outputs.controlplane-stack }}"
            "${{ needs.setup.outputs.worker-stack }}"
            "${{ needs.setup.outputs.monitoring-worker-stack }}"
            "${{ needs.setup.outputs.appiam-stack }}"
            "${{ needs.setup.outputs.edge-stack }}"
          )

          for STACK in "${STACKS[@]}"; do
            [ -z "$STACK" ] && continue
            echo "### Stack: \`${STACK}\`" >> $GITHUB_STEP_SUMMARY
            EVENTS=$(aws cloudformation describe-stack-events \
              --stack-name "$STACK" \
              --query 'StackEvents[?contains(ResourceStatus,`FAILED`)].[Timestamp,LogicalResourceId,ResourceStatusReason]' \
              --output table --region ${{ env.AWS_REGION }} 2>/dev/null)
            if [ -n "$EVENTS" ]; then
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "$EVENTS" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "No failed events" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          done

          # K8s pod diagnostics via SSM (best-effort)
          INSTANCE_ID=$(aws ssm get-parameter \
            --name "/k8s/${{ inputs.cdk-environment }}/instance-id" \
            --query 'Parameter.Value' --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ -n "$INSTANCE_ID" ]; then
            echo "### K8s Pod Status (non-Running)" >> $GITHUB_STEP_SUMMARY
            CMD_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["sudo kubectl get pods -A --field-selector=status.phase!=Running -o wide 2>/dev/null || echo No kubectl access"]' \
              --query 'Command.CommandId' --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
            if [ -n "$CMD_ID" ]; then
              sleep 10
              POD_OUTPUT=$(aws ssm get-command-invocation \
                --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" \
                --query 'StandardOutputContent' --output text \
                --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Could not retrieve")
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "$POD_OUTPUT" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "_Could not resolve instance ID from SSM — skipping K8s diagnostics_" >> $GITHUB_STEP_SUMMARY
          fi

      # -----------------------------------------------------------------------
      # Boot Logs — fetch recent CloudWatch logs from the EC2 instance
      # -----------------------------------------------------------------------
      - name: "Fetch Boot Logs from CloudWatch"
        if: failure()
        continue-on-error: true
        run: just ci-fetch-boot-logs ${{ inputs.cdk-environment }} --region ${{ env.AWS_REGION }}

  # ===========================================================================
  # Deployment Failure Alert (staging/production only)
  #
  # For K8s/GitOps environments, automated CFn rollbacks on stateful infra
  # (EC2 nodes running ArgoCD) risk split-brain state. Instead we:
  #   1. Halt the pipeline
  #   2. Preserve artifacts and diagnostics
  #   3. Alert engineers with actionable context
  #   4. Let them "roll forward" with a targeted fix
  # ===========================================================================
  deployment-failure-alert:
    name: "Deployment Failed — Alert"
    needs:
      [
        setup,
        deploy-controlplane,
        deploy-app-worker,
        deploy-monitoring-worker,
        deploy-edge,
        verify-and-test,
      ]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    environment: ${{ inputs.environment }}
    if: |
      always()
      && (
        needs.deploy-controlplane.result == 'failure'
        || needs.deploy-app-worker.result == 'failure'
        || needs.deploy-monitoring-worker.result == 'failure'
        || needs.deploy-edge.result == 'failure'
        || needs.verify-and-test.result == 'failure'
      )
      && inputs.cdk-environment != 'development'

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: "Collect CloudFormation Diagnostics"
        continue-on-error: true
        run: |
          echo "## Deployment Failure Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Field | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Environment** | ${{ inputs.environment }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Commit** | \`${{ needs.setup.outputs.commit-sha }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| **Control Plane** | ${{ needs.deploy-controlplane.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **App Worker** | ${{ needs.deploy-app-worker.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Mon. Worker** | ${{ needs.deploy-monitoring-worker.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Edge** | ${{ needs.deploy-edge.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Verification** | ${{ needs.verify-and-test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### CloudFormation Failed Events" >> $GITHUB_STEP_SUMMARY
          STACKS=(
            "${{ needs.setup.outputs.data-stack }}"
            "${{ needs.setup.outputs.base-stack }}"
            "${{ needs.setup.outputs.controlplane-stack }}"
            "${{ needs.setup.outputs.worker-stack }}"
            "${{ needs.setup.outputs.monitoring-worker-stack }}"
            "${{ needs.setup.outputs.appiam-stack }}"
            "${{ needs.setup.outputs.edge-stack }}"
          )

          for STACK in "${STACKS[@]}"; do
            [ -z "$STACK" ] && continue
            echo "#### \`${STACK}\`" >> $GITHUB_STEP_SUMMARY
            EVENTS=$(aws cloudformation describe-stack-events \
              --stack-name "$STACK" \
              --query 'StackEvents[?contains(ResourceStatus,`FAILED`)].[Timestamp,LogicalResourceId,ResourceStatusReason]' \
              --output table --region ${{ env.AWS_REGION }} 2>/dev/null)
            if [ -n "$EVENTS" ]; then
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "$EVENTS" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ No failed events" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          done

      - name: "Collect K8s Diagnostics via SSM"
        continue-on-error: true
        run: |
          INSTANCE_ID=$(aws ssm get-parameter \
            --name "/k8s/${{ inputs.cdk-environment }}/instance-id" \
            --query 'Parameter.Value' --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ -z "$INSTANCE_ID" ]; then
            echo "### K8s Diagnostics" >> $GITHUB_STEP_SUMMARY
            echo "_Could not resolve instance ID — skipping_" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          echo "### K8s Pod Status (non-Running)" >> $GITHUB_STEP_SUMMARY
          CMD_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["sudo kubectl get pods -A --field-selector=status.phase!=Running -o wide 2>/dev/null || echo No kubectl access"]' \
            --query 'Command.CommandId' --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ -n "$CMD_ID" ]; then
            sleep 15
            POD_OUTPUT=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" \
              --query 'StandardOutputContent' --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Could not retrieve pod status")
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$POD_OUTPUT" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: "Fetch Boot Logs from CloudWatch"
        continue-on-error: true
        run: |
          echo "### EC2 Boot Logs (last 15 minutes)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          LOG_GROUP="/ec2/k8s-${{ inputs.cdk-environment }}/instances"
          START_TIME=$(( $(date +%s) * 1000 - 900000 ))
          EVENTS=$(aws logs filter-log-events \
            --log-group-name "$LOG_GROUP" \
            --start-time "$START_TIME" \
            --query 'events[].{ts:timestamp,stream:logStreamName,msg:message}' \
            --output table \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          if [ -n "$EVENTS" ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$EVENTS" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "_No boot log events found (log group: ${LOG_GROUP})_" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: "Post Failure Context"
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Quick Links" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **[Failed Run Logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})**" >> $GITHUB_STEP_SUMMARY
          echo "- **[Commit](${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }})**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> **Do not run automated CFn rollbacks on K8s/GitOps infrastructure.**" >> $GITHUB_STEP_SUMMARY
          echo "> Prefer \"roll forward\" — push a fix commit to trigger a new deployment." >> $GITHUB_STEP_SUMMARY

          echo "::error::Deployment verification failed for ${{ inputs.environment }}. Check the Deployment Failure Report in the job summary for diagnostics."

  # ===========================================================================
  # Deployment Summary (TypeScript)
  # ===========================================================================
  summary:
    name: Deployment Summary
    needs:
      - setup
      - security-scan
      - drift-detection
      - deploy-data
      - deploy-controlplane
      - build-golden-ami
      - deploy-app-worker
      - deploy-monitoring-worker
      - deploy-edge
      - sync-static-assets
      - verify-and-test
      - deployment-failure-alert
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()

    steps:
      # Guard: if setup was cancelled/failed, write minimal fallback
      - name: Check Prerequisites
        id: prereqs
        run: |
          if [ "${{ needs.setup.result }}" != "success" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## Pipeline Aborted

          **Reason**: Setup job did not complete successfully (`${{ needs.setup.result }}`)
          **Commit**: `${{ github.sha }}`
          **Environment**: ${{ inputs.environment }}

          No stacks were deployed. Check the setup job logs for details.
          EOF
            echo "::warning::Setup did not succeed (${{ needs.setup.result }}), skipping full summary"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout
        if: steps.prereqs.outputs.skip != 'true'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Setup Node.js and Yarn
        if: steps.prereqs.outputs.skip != 'true'
        uses: ./.github/actions/setup-node-yarn
        with:
          node-version: ${{ needs.setup.outputs.node-version }}

      - name: Install Dependencies
        if: steps.prereqs.outputs.skip != 'true'
        run: yarn install --frozen-lockfile
        working-directory: infra

      - name: Generate Summary
        if: steps.prereqs.outputs.skip != 'true'
        env:
          DEPLOY_DATA_RESULT: ${{ needs.deploy-data.result }}
          DEPLOY_CONTROLPLANE_RESULT: ${{ needs.deploy-controlplane.result }}
          DEPLOY_WORKER_RESULT: ${{ needs.deploy-app-worker.result || 'skipped' }}
          DEPLOY_MON_WORKER_RESULT: ${{ needs.deploy-monitoring-worker.result || 'skipped' }}
          DEPLOY_EDGE_RESULT: ${{ needs.deploy-edge.result || 'skipped' }}
          SECURITY_SCAN_RESULT: ${{ needs.security-scan.result || 'skipped' }}
          VERIFY_RESULT: ${{ needs.verify-and-test.result || 'skipped' }}
          SMOKE_TESTS_RESULT: ${{ needs.verify-and-test.result || 'skipped' }}
          ALERT_RESULT: ${{ needs.deployment-failure-alert.result || 'skipped' }}
          COMMIT_SHORT_SHA: ${{ needs.setup.outputs.commit-short-sha }}
        run: just ci-summary kubernetes ${{ inputs.cdk-environment }}
